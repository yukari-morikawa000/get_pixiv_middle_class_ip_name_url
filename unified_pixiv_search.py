# unified_pixiv_search.py

from playwright.sync_api import sync_playwright # type: ignore
import urllib.parse
from google.cloud import bigquery
from datetime import datetime
import pandas as pd
import time
import random
import unicodedata
import re # æ­£è¦è¡¨ç¾
import os

# middle_class_ip_nameã‚’æ•´å½¢
from clean_name import clean_ip_name 

# digãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ç’°å¢ƒå¤‰æ•°
PROJECT_ID = os.environ.get("GCP_PROJECT_ID")
DATASET_ID = os.environ.get("BIGQUERY_DATASET")

# ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã‹ã£ãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼å‡¦ç†
if not PROJECT_ID or not DATASET_ID:
    print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° GCP_PROJECT_ID ã¾ãŸã¯ BIGQUERY_DATASET ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print(f"GCP_PROJECT_ID: {PROJECT_ID}")
    print(f"BIGQUERY_DATASET: {DATASET_ID}")
    raise ValueError("å¿…é ˆã®ç’°å¢ƒå¤‰æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")

# ãƒã‚¹ã‚¿ãƒ¼IPãƒªã‚¹ãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«
MASTER_TABLE = "master_spreadsheet.master_ip_list_twitter_account_tb"
MASTER_COLUMN = "ip_name" 
# ã™ã§ã«ãƒ”ã‚¯ã‚·ãƒ–ç™¾ç§‘äº‹å…¸ã‹ã‚‰URLã‚’å–å¾—æ¸ˆã¿ã®ãƒ†ãƒ¼ãƒ–ãƒ«
PROCESSED_TABLE = f"{DATASET_ID}.pixiv_search_middle_class_ip_name_url"
PROCESSED_COLUMN = "middle_class_ip_name" 
# çµæœã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«
BQ_TARGET_TABLE = f"{DATASET_ID}.pixiv_search_results"

# å…¨è§’åŠè§’ã®åˆ¤åˆ¥
def normalize(text):
    return unicodedata.normalize("NFKC", text.strip().lower())

# BigQueryã§ã®å‡¦ç†
def get_bq_names(table_id: str, column_name: str) -> set:

    print(f"BigQuery: {table_id} ã‹ã‚‰ {column_name} ã‚’å–å¾—ä¸­...")
    try:
        client = bigquery.Client(project=PROJECT_ID)
        query = f"SELECT DISTINCT {column_name} FROM `{PROJECT_ID}.{table_id}`"
        df = client.query(query).to_dataframe()
        
        # NaNã‚’é™¤å¤–ã—ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ–‡å­—åˆ—ã®ã‚»ãƒƒãƒˆã‚’è¿”ã™
        unique_names = set(df[column_name].dropna().astype(str))
        print(f"-> {table_id} ã‹ã‚‰ {len(unique_names)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        return unique_names
        
    except Exception as e:
        print(f"BigQueryã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return set()

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    print("--- BigQueryå·®åˆ†æŠ½å‡ºãƒ»Pixivæ¤œç´¢ãƒ»BQãƒ­ãƒ¼ãƒ‰å‡¦ç† é–‹å§‹ ---")
    
    bq_client = bigquery.Client(project=PROJECT_ID)
    
    # BQã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    master_names = get_bq_names(MASTER_TABLE, MASTER_COLUMN)
    processed_names = get_bq_names(PROCESSED_TABLE, PROCESSED_COLUMN)

    # å·®åˆ†ã‚’æŠ½å‡º
    new_names = master_names - processed_names

    if not new_names:
        print("\nâœ¨ æ–°è¦ã«è¿½åŠ ãƒ»æœªå‡¦ç†ã®IPåã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    print("-" * 30)
    print(f"âœ¨ {len(new_names)} ä»¶ã®æ–°è¦/æœªå‡¦ç†ã®IPåãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    
    # æ¤œç´¢ã‚¿ã‚¹ã‚¯ã®æº–å‚™ã¨æ•´å½¢
    tasks = []
    for original_name in sorted(list(new_names)):
        cleaned_name = clean_ip_name(original_name)
        if cleaned_name:
            tasks.append({
                "original_name": original_name,
                "cleaned_name": cleaned_name
            })
            
    if not tasks:
        print("æ•´å½¢å¾Œã€å‡¦ç†å¯¾è±¡ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
        
    print(f"æ•´å½¢å¾Œã€æ¤œç´¢å¯¾è±¡ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿: {len(tasks)} ä»¶")

    # --- ãƒ†ã‚¹ãƒˆç”¨ï¼ˆæœ¬ç•ªå®Ÿè¡Œã®å ´åˆã¯ã“ã“ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã™ã‚‹ï¼‰ ----
    # æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§1å›30ä»¶ã¾ã§ã«åˆ¶é™
    TEST_LIMIT = 5
    if len(tasks) > TEST_LIMIT:
        tasks = tasks[:TEST_LIMIT]
        print(f"ãƒ†ã‚¹ãƒˆã®ãŸã‚ã€æ¤œç´¢å¯¾è±¡ã‚’å…ˆé ­ã® {TEST_LIMIT} ä»¶ã«åˆ¶é™ã—ã¾ã™ã€‚")
    # --- ã“ã“ã¾ã§ ------

    # Pixivæ¤œç´¢ã¨BQãƒ‡ãƒ¼ã‚¿æº–å‚™
    rows_to_insert = []
    
    try:
        with sync_playwright() as p:
            print("Playwright ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­...")
            browser = p.chromium.launch(headless=True)
            
            for i, task in enumerate(tasks):
                original_name = task['original_name']
                cleaned_name = task['cleaned_name']
                
                print(f"({i + 1}/{len(tasks)}) æ¤œç´¢: {cleaned_name} (å…ƒ: {original_name})")

                try:
                    candidates = []
                    hit_count = 0 
                    today_date = datetime.now().date().isoformat()
                    
                    page = browser.new_page()
                    
                    keyword = cleaned_name
                    search_url = "https://dic.pixiv.net/search?query=" + urllib.parse.quote(keyword)
                    
                    page.goto(search_url, timeout=60000)

                    # ä»¶æ•°å–å¾—ãƒ­ã‚¸ãƒƒã‚¯
                    try:
                        info_locator = page.locator("header .info")
                        if info_locator.count() > 0:
                            info_text = info_locator.nth(0).inner_text().strip()
                            m = re.search(r"æ¤œç´¢çµæœï¼š(\d+)ä»¶", info_text)
                            if m: hit_count = int(m.group(1))
                    except Exception as e:
                        print(f"ä»¶æ•°å–å¾—å¤±æ•—: {e}")

                    # å®Œå…¨ä¸€è‡´ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã‚’å–å¾—ãƒ­ã‚¸ãƒƒã‚¯
                    articles = page.locator("article")
                    count = articles.count()
                    for j in range(count):
                        title_el = articles.nth(j).locator("div.info h2 a")
                        if title_el.count() == 0: continue

                        title = title_el.inner_text().strip()
                        href = title_el.get_attribute("href")
                        
                        if normalize(title) == normalize(keyword): 
                            full_url = urllib.parse.urljoin("https://dic.pixiv.net", href)
                            candidates.append((title, full_url))
                    
                    # ãƒšãƒ¼ã‚¸ã‚’é–‰ã˜ã‚‹
                    page.close() 

                    if candidates:
                        for title, link in candidates:
                            print(f"  ãƒ’ãƒƒãƒˆ: {title} â†’ {link}")
                            rows_to_insert.append({
                                "middle_class_ip_name": original_name,
                                "clean_middle_class_ip_name": cleaned_name,
                                "URL": link,
                                "pixiv_search_result_ip": title,
                                "search_hit_num": None,
                                "update": today_date,
                            })
                    else:
                        print(f"è©²å½“ãªã— (æ¤œç´¢çµæœä»¶æ•°: {hit_count}ä»¶)")
                        rows_to_insert.append({
                            "middle_class_ip_name": original_name,
                            "clean_middle_class_ip_name": cleaned_name,
                            "URL": "",
                            "pixiv_search_result_ip": "è©²å½“ãªã—",
                            "search_hit_num": hit_count,
                            "update": today_date,
                        })

                except Exception as e:
                    # ãƒšãƒ¼ã‚¸æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
                    print(f"  é‡å¤§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆPlaywright/BigQueryé€ä¿¡å¯¾è±¡ï¼‰ï¼š{e}")
                    rows_to_insert.append({
                        "middle_class_ip_name": original_name,
                        "clean_middle_class_ip_name": cleaned_name,
                        "URL": str(e),
                        "pixiv_search_result_ip": "ERROR",
                        "search_hit_num": -1,
                        "update": datetime.now().date().isoformat(),
                    })
                
                # ã‚µã‚¤ãƒˆã¸ã®è² è·ã‚’è€ƒæ…®ã—ã€å‡¦ç†ã”ã¨ã«å¾…æ©Ÿ
                time.sleep(random.uniform(5, 10)) 
            
            # å…¨å‡¦ç†çµ‚äº†å¾Œã€ãƒ–ãƒ©ã‚¦ã‚¶ã‚’çµ‚äº†
            browser.close()
            print("Playwright ãƒ–ãƒ©ã‚¦ã‚¶ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        # Playwright/ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ãã®ã‚‚ã®ã«é–¢ã™ã‚‹é‡å¤§ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒ
        print(f"Playwright å…¨ä½“ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ã¾ãŸã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ï¼‰ï¼š{e}")
        # ç’°å¢ƒã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’é˜²ããŸã‚ã€å†ã‚¹ãƒ­ãƒ¼ã™ã‚‹
        raise 
        
    # BigQueryã«ä¸€æ‹¬é€ä¿¡ (å…¨ä»¶æ´—ã„æ›¿ãˆ WRITE_TRUNCATE ã‚’ä½¿ç”¨)
    if rows_to_insert:
        print("-" * 30)
        print(f"åˆè¨ˆ {len(rows_to_insert)} ä»¶ã®æ¤œç´¢çµæœã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        
        # æŒ¿å…¥ç”¨ã®DataFrameã«å¤‰æ›
        result_df = pd.DataFrame(rows_to_insert)
        
        job_config = bigquery.LoadJobConfig(
            # æ›´æ–°ã®éš›ã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ä¸Šæ›¸ã
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        )
        
        try:
            # DataFrameã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰
            job = bq_client.load_table_from_dataframe(
                result_df, BQ_TARGET_TABLE, job_config=job_config
            )
            job.result()  # ãƒ­ãƒ¼ãƒ‰ã‚¸ãƒ§ãƒ–ã®å®Œäº†ã‚’å¾…æ©Ÿ
            
            print(f"ğŸ‰ BigQueryä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ« ({BQ_TARGET_TABLE}) ã¸ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            print(f" (ãƒ†ãƒ¼ãƒ–ãƒ«ã¯å…¨ {len(result_df)} ä»¶ã§ä¸Šæ›¸ãã•ã‚Œã¾ã—ãŸ)")
            
        except Exception as e:
            print(f" BigQueryã¸ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            
    else:
        print("æ¤œç´¢çµæœãŒå¾—ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€BigQueryã¸ã®ãƒ­ãƒ¼ãƒ‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()