# unified_pixiv_search.py

from playwright.sync_api import sync_playwright # type: ignore
import urllib.parse
from google.cloud import bigquery
from datetime import datetime
import pandas as pd
import time
import random
import unicodedata
import re # æ­£è¦è¡¨ç¾
import os

# middle_class_ip_nameã‚’æ•´å½¢
from clean_name import clean_ip_name 

# PROJECT_ID = "hogeticlab-legs-prd"
# DATASET_ID = "z_personal_morikawa"  
# digãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ç’°å¢ƒå¤‰æ•°
PROJECT_ID = os.environ.get("GCP_PROJECT_ID")
DATASET_ID = os.environ.get("BIGQUERY_DATASET")

# ãƒã‚¹ã‚¿ãƒ¼IPãƒªã‚¹ãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«
MASTER_TABLE = "master_spreadsheet.master_ip_list_twitter_account_tb"
MASTER_COLUMN = "ip_name" 
# ã™ã§ã«ãƒ”ã‚¯ã‚·ãƒ–ç™¾ç§‘äº‹å…¸ã‹ã‚‰URLã‚’å–å¾—æ¸ˆã¿ã®ãƒ†ãƒ¼ãƒ–ãƒ«
PROCESSED_TABLE = f"{DATASET_ID}.pixiv_search_middle_class_ip_name_url"
PROCESSED_COLUMN = "middle_class_ip_name" 
# çµæœã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«
BQ_TARGET_TABLE = f"{DATASET_ID}.pixiv_search_results"

# å…¨è§’åŠè§’ã®åˆ¤åˆ¥
def normalize(text):
    return unicodedata.normalize("NFKC", text.strip().lower())

# BigQueryã§ã®å‡¦ç†
def get_bq_names(table_id: str, column_name: str) -> set:

    print(f"BigQuery: {table_id} ã‹ã‚‰ {column_name} ã‚’å–å¾—ä¸­...")
    try:
        client = bigquery.Client(project=PROJECT_ID)
        query = f"SELECT DISTINCT {column_name} FROM `{PROJECT_ID}.{table_id}`"
        df = client.query(query).to_dataframe()
        
        # NaNã‚’é™¤å¤–ã—ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ–‡å­—åˆ—ã®ã‚»ãƒƒãƒˆã‚’è¿”ã™
        unique_names = set(df[column_name].dropna().astype(str))
        print(f"-> {table_id} ã‹ã‚‰ {len(unique_names)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        return unique_names
        
    except Exception as e:
        print(f"BigQueryã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return set()

#  Pixivæ¤œç´¢é–¢æ•°
def search_pixiv_dic(keyword):
    """
    Playwrightã§Pixivç™¾ç§‘äº‹å…¸ã‚’æ¤œç´¢ã—ã€å®Œå…¨ä¸€è‡´ã™ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã€
    ãŠã‚ˆã³æ¤œç´¢çµæœã®ä»¶æ•°ã‚’å–å¾—ã™ã‚‹ã€‚
    """
    search_url = "https://dic.pixiv.net/search?query=" + urllib.parse.quote(keyword)
    results = []
    hit_count = 0 # ä»¶æ•°ã‚«ã‚¦ãƒ³ãƒˆ
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                page.goto(search_url, timeout=60000)

                # ä»¶æ•°å–å¾—
                try:
                    info_locator = page.locator("header .info")
                    if info_locator.count() > 0:
                        info_text = info_locator.nth(0).inner_text().strip()
                        # 'æ¤œç´¢çµæœï¼šXXXä»¶' ã®å½¢å¼ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
                        m = re.search(r"æ¤œç´¢çµæœï¼š(\d+)ä»¶", info_text)
                        if m:
                            hit_count = int(m.group(1))
                except Exception as e:
                    print(f"ä»¶æ•°å–å¾—å¤±æ•—: {e}")

                # å®Œå…¨ä¸€è‡´ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã‚’å–å¾—
                articles = page.locator("article")
                count = articles.count()
                for i in range(count):
                    title_el = articles.nth(i).locator("div.info h2 a")
                    if title_el.count() == 0:
                        continue

                    title = title_el.inner_text().strip()
                    href = title_el.get_attribute("href")
                    
                    if normalize(title) == normalize(keyword): 
                        full_url = urllib.parse.urljoin("https://dic.pixiv.net", href)
                        results.append((title, full_url))
                
                browser.close()

            except Exception as e:
                print(f"Playwright/æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                return [], 0 # å†…éƒ¨ã‚¨ãƒ©ãƒ¼æ™‚ã€çµæœã¨ä»¶æ•°0ã‚’è¿”ã™
            
    except Exception as e:
        print(f"å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
        return [], 0 # å…¨ä½“ã‚¨ãƒ©ãƒ¼æ™‚ã€çµæœã¨ä»¶æ•°0ã‚’è¿”ã™

    return results, hit_count # çµæœã¨ä»¶æ•°ã‚’è¿”ã™

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    print("--- BigQueryå·®åˆ†æŠ½å‡ºãƒ»Pixivæ¤œç´¢ãƒ»BQãƒ­ãƒ¼ãƒ‰å‡¦ç† é–‹å§‹ ---")
    
    bq_client = bigquery.Client(project=PROJECT_ID)
    
    # BQã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    master_names = get_bq_names(MASTER_TABLE, MASTER_COLUMN)
    processed_names = get_bq_names(PROCESSED_TABLE, PROCESSED_COLUMN)

    # å·®åˆ†ã‚’æŠ½å‡º
    new_names = master_names - processed_names

    if not new_names:
        print("\nâœ¨ æ–°è¦ã«è¿½åŠ ãƒ»æœªå‡¦ç†ã®IPåã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    print("-" * 30)
    print(f"âœ¨ {len(new_names)} ä»¶ã®æ–°è¦/æœªå‡¦ç†ã®IPåãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    
    # æ¤œç´¢ã‚¿ã‚¹ã‚¯ã®æº–å‚™ã¨æ•´å½¢
    tasks = []
    for original_name in sorted(list(new_names)):
        cleaned_name = clean_ip_name(original_name)
        if cleaned_name:
            tasks.append({
                "original_name": original_name,
                "cleaned_name": cleaned_name
            })
            
    if not tasks:
        print("æ•´å½¢å¾Œã€å‡¦ç†å¯¾è±¡ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
        
    print(f"æ•´å½¢å¾Œã€æ¤œç´¢å¯¾è±¡ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿: {len(tasks)} ä»¶")

    # --- ãƒ†ã‚¹ãƒˆç”¨ï¼ˆæœ¬ç•ªå®Ÿè¡Œã®å ´åˆã¯ã“ã“ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã™ã‚‹ï¼‰ ----
    TEST_LIMIT = 5
    if len(tasks) > TEST_LIMIT:
        tasks = tasks[:TEST_LIMIT]
        print(f"ãƒ†ã‚¹ãƒˆã®ãŸã‚ã€æ¤œç´¢å¯¾è±¡ã‚’å…ˆé ­ã® {TEST_LIMIT} ä»¶ã«åˆ¶é™ã—ã¾ã™ã€‚")
    # --- ã“ã“ã¾ã§ ------

    # Pixivæ¤œç´¢ã¨BQãƒ‡ãƒ¼ã‚¿æº–å‚™
    rows_to_insert = []
    
    for i, task in enumerate(tasks):
        original_name = task['original_name']
        cleaned_name = task['cleaned_name']
        
        print(f"({i + 1}/{len(tasks)}) æ¤œç´¢: {cleaned_name} (å…ƒ: {original_name})")

        try:
            # ä»¶æ•°ã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
            candidates, hit_count = search_pixiv_dic(cleaned_name) 
            # æ›´æ–°æ—¥
            today_date = datetime.now().date().isoformat()
            
            if candidates:
                for title, link in candidates:
                    print(f"  ãƒ’ãƒƒãƒˆ: {title} â†’ {link}")
                    rows_to_insert.append({
                        "middle_class_ip_name": original_name,
                        "clean_middle_class_ip_name": cleaned_name,
                        "URL": link,
                        "pixiv_search_result_ip": title,
                        "search_hit_num": None, # ãƒ’ãƒƒãƒˆã™ã‚‹å ´åˆã¯å¿…è¦ãªã—
                        "update": today_date,
                    })
            else:
                print(f"è©²å½“ãªã— (æ¤œç´¢çµæœä»¶æ•°: {hit_count}ä»¶)")
                rows_to_insert.append({
                    "middle_class_ip_name": original_name,
                    "clean_middle_class_ip_name": cleaned_name,
                    "URL": "",
                    "pixiv_search_result_ip": "è©²å½“ãªã—",
                    "search_hit_num": hit_count, # ä»¶æ•°ã‚’è¿½åŠ 
                    "update": today_date,
                })

        except Exception as e:
            print(f"  é‡å¤§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆBigQueryé€ä¿¡å¯¾è±¡ï¼‰ï¼š{e}")
            rows_to_insert.append({
                "middle_class_ip_name": original_name,
                "clean_middle_class_ip_name": cleaned_name,
                "URL": str(e),
                "pixiv_search_result_ip": "ERROR",
                "search_hit_num": -1, # ã‚¨ãƒ©ãƒ¼æ™‚ã¯-1ãªã©ã‚’è¨­å®š
                "update": today_date,
            })

        # ã‚µã‚¤ãƒˆã¸ã®è² è·ã‚’è€ƒæ…®ã—ã€å‡¦ç†ã”ã¨ã«å¾…æ©Ÿ
        time.sleep(random.uniform(5, 10)) 

 # BigQueryã«ä¸€æ‹¬é€ä¿¡ (å…¨ä»¶æ´—ã„æ›¿ãˆ WRITE_TRUNCATE ã‚’ä½¿ç”¨)
    if rows_to_insert:
        print("-" * 30)
        print(f"åˆè¨ˆ {len(rows_to_insert)} ä»¶ã®æ¤œç´¢çµæœã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        
        # æŒ¿å…¥ç”¨ã®DataFrameã«å¤‰æ›
        result_df = pd.DataFrame(rows_to_insert)
        
        job_config = bigquery.LoadJobConfig(
            # æ›´æ–°ã®éš›ã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ä¸Šæ›¸ã
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        )
        
        try:
            # DataFrameã‚’BigQueryã«ãƒ­ãƒ¼ãƒ‰
            job = bq_client.load_table_from_dataframe(
                result_df, BQ_TARGET_TABLE, job_config=job_config
            )
            job.result()  # ãƒ­ãƒ¼ãƒ‰ã‚¸ãƒ§ãƒ–ã®å®Œäº†ã‚’å¾…æ©Ÿ
            
            print(f"ğŸ‰ BigQueryä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ« ({BQ_TARGET_TABLE}) ã¸ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            print(f" (ãƒ†ãƒ¼ãƒ–ãƒ«ã¯å…¨ {len(result_df)} ä»¶ã§ä¸Šæ›¸ãã•ã‚Œã¾ã—ãŸ)")
            
        except Exception as e:
            print(f" BigQueryã¸ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            
    else:
        print("æ¤œç´¢çµæœãŒå¾—ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€BigQueryã¸ã®ãƒ­ãƒ¼ãƒ‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()